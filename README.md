1. Neural Networks - Build, train, and apply fully connected deep neural networks; implement efficient (vectorized) neural networks; identify key parameters in a neural networkâ€™s architecture.

2. Improving NNs (tuning, regularization, and optimization) - Open the black box to understand the processes that drive performance and generate good results systematically. Train and develop test sets and analyze bias/variance for building deep learning applications; use standard neural network techniques such as initialization, L2 and dropout regularization, hyperparameter tuning, batch normalization, and gradient checking; implement and apply a variety of optimization algorithms, such as mini-batch gradient descent, Momentum, RMSprop and Adam, and check for their convergence; implement a neural network in TensorFlow.

3. Structuring projects - Diagnose errors in a machine learning system; prioritize strategies for reducing errors; understand complex ML settings, such as mismatched training/test sets, and comparing to and/or surpassing human-level performance; and apply end-to-end learning, transfer learning, and multi-task learning. Become a technical leader who can set the direction for an AI team.

4. CNNs - Understand how computer vision has evolved and become familiar with its applications such as autonomous driving, face recognition, and reading radiology images. Build a convolutional neural network, including recent variations such as residual networks; apply convolutional networks to visual detection and recognition tasks; and use neural style transfer to generate art and apply these algorithms to a variety of image, video, and other 2D or 3D data. 

5. Seuence models - Become familiar with sequence models and their applications such as speech recognition, music synthesis, chatbots, machine translation, and natural language processing (NLP). Build and train Recurrent Neural Networks (RNNs) and commonly-used variants such as GRUs and LSTMs; apply RNNs to Character-level Language Modeling; gain experience with natural language processing and Word Embeddings; and use HuggingFace tokenizers and transformer models to solve different NLP tasks such as NER and Question Answering.